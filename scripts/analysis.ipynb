{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8dfcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2ab761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/22 23:35:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"mkt_analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a CSV file with header and infer schema\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .csv(\"../data/Advertising_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e73b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+------------+--------------------+-------------------+------------+\n",
      "|    TV|Billboards|Google_Ads|Social_Media|Influencer_Marketing|Affiliate_Marketing|Product_Sold|\n",
      "+------+----------+----------+------------+--------------------+-------------------+------------+\n",
      "|281.42|     538.8|    123.94|       349.3|              242.77|              910.1|      7164.0|\n",
      "|702.97|    296.53|    558.13|      180.55|              781.06|             132.43|      5055.0|\n",
      "|313.14|    295.94|    642.96|      505.71|              438.91|             464.23|      6154.0|\n",
      "|898.52|     61.27|    548.73|      240.93|              278.96|             432.27|      5480.0|\n",
      "|766.52|    550.72|    651.91|      666.33|              396.33|             841.93|      9669.0|\n",
      "+------+----------+----------+------------+--------------------+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns (excluding Product_Sold which is the target)\n",
    "feature_cols = [\"TV\", \"Billboards\", \"Google_Ads\", \"Social_Media\", \n",
    "                \"Influencer_Marketing\", \"Affiliate_Marketing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165387e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[281.42,538.8,123...|\n",
      "|[702.97,296.53,55...|\n",
      "|[313.14,295.94,64...|\n",
      "|[898.52,61.27,548...|\n",
      "|[766.52,550.72,65...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Demonstration of generated vectors...\n",
    "output = assembler.transform(df)\n",
    "output.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69082d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|scaled_features                                                                                                            |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-0.8191561355470773,0.13107147661820287,-1.3611551377860516,-0.5129914254811337,-0.7733322182187209,1.5314301427984658]   |\n",
      "|[0.643975959736068,-0.7472199930962898,0.1600640683124651,-1.1291283752053067,1.0936941888072516,-1.2663922339352824]      |\n",
      "|[-0.7090611288755525,-0.7493588958375765,0.4572727217898304,0.058089939297283226,-0.09303245512805543,-0.07267579302813974]|\n",
      "|[1.3226984866785874,-1.6000983980371886,0.12713042051466686,-0.9086700106165922,-0.6478093854143405,-0.18765824465982248]  |\n",
      "|[0.8645477653468944,0.17428456250996338,0.4886297587887977,0.6445427799769673,-0.24071861762627383,1.2861750199032278]     |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Standardize the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "# Demonstration of scaled features...\n",
    "scaled_output = scaler.fit(output).transform(output)\n",
    "scaled_output.select(\"scaled_features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a35155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2: Total explained variance = 0.3772\n",
      "  Individual variances: [0.20029233 0.17685879]\n",
      "\n",
      "k=3: Total explained variance = 0.5481\n",
      "  Individual variances: [0.20029233 0.17685879 0.17090627]\n",
      "\n",
      "k=4: Total explained variance = 0.7104\n",
      "  Individual variances: [0.20029233 0.17685879 0.17090627 0.16233868]\n",
      "\n",
      "k=5: Total explained variance = 0.8664\n",
      "  Individual variances: [0.20029233 0.17685879 0.17090627 0.16233868 0.15597116]\n",
      "\n",
      "k=6: Total explained variance = 1.0000\n",
      "  Individual variances: [0.20029233 0.17685879 0.17090627 0.16233868 0.15597116 0.13363277]\n",
      "\n",
      "Optimal k for 95.0% variance: 6\n",
      "\n",
      "Variance contribution by each component:\n",
      "Adding component 2: marginal variance = 0.2003\n",
      "Adding component 3: marginal variance = 0.1709\n",
      "Adding component 4: marginal variance = 0.1623\n",
      "Adding component 5: marginal variance = 0.1560\n",
      "Adding component 6: marginal variance = 0.1336\n",
      "\n",
      "Using k=3 components for final PCA\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Apply PCA\n",
    "# Step 3: Apply PCA with component exploration\n",
    "\n",
    "# Method 1: Try different k values and analyze explained variance\n",
    "k_values = [2, 3, 4, 5, 6]  # All possible values since we have 6 features\n",
    "explained_variances = []\n",
    "\n",
    "for k in k_values:\n",
    "    pca_temp = PCA(k=k, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "    pca_model = pca_temp.fit(scaled_output)\n",
    "    explained_var = pca_model.explainedVariance.toArray()\n",
    "    total_explained = sum(explained_var)\n",
    "    explained_variances.append((k, total_explained, explained_var))\n",
    "    print(f\"k={k}: Total explained variance = {total_explained:.4f}\")\n",
    "    print(f\"  Individual variances: {explained_var}\")\n",
    "    print()\n",
    "\n",
    "# Method 2: Find optimal k based on cumulative explained variance threshold\n",
    "cumulative_variance = 0\n",
    "optimal_k = 2\n",
    "variance_threshold = 0.95  # Capture 95% of variance\n",
    "\n",
    "for k, total_var, individual_vars in explained_variances:\n",
    "    if total_var >= variance_threshold:\n",
    "        optimal_k = k\n",
    "        break\n",
    "\n",
    "print(f\"Optimal k for {variance_threshold*100}% variance: {optimal_k}\")\n",
    "\n",
    "# Method 3: Elbow method - look for diminishing returns\n",
    "print(\"\\nVariance contribution by each component:\")\n",
    "for k, total_var, individual_vars in explained_variances:\n",
    "    if len(individual_vars) > 1:\n",
    "        marginal_contribution = individual_vars[-1] if k > 2 else individual_vars[0]\n",
    "        print(f\"Adding component {k}: marginal variance = {marginal_contribution:.4f}\")\n",
    "\n",
    "# Final PCA with chosen k (using optimal_k or manual selection)\n",
    "k_selected = 3  # You can change this to optimal_k or any preferred value\n",
    "pca = PCA(k=k_selected, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "print(f\"\\nUsing k={k_selected} components for final PCA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create and fit the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, pca])\n",
    "model = pipeline.fit(df)\n",
    "result = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "result.select(\"pca_features\").show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
